# RAG PDF Question-Answering with LLaMA

This project demonstrates how to build a **Retrieval-Augmented Generation (RAG) pipeline** for PDF documents using:

- **LLaMA 3.2** (local GGUF model)
- **SentenceTransformers** for embeddings
- **Chroma** vector database
- **Streamlit** for an interactive web interface
- **PyPDF2** for PDF reading

Users can ask questions about a PDF, and the system retrieves relevant content and generates answers.

---
## ğŸ” RAG Workflow

![RAG Workflow](assets/flowchart.png)
## Features

- Load and split PDF documents into smaller chunks
- Create embeddings with `SentenceTransformer`
- Store embeddings in a persistent **Chroma vector database**
- Query PDF content interactively using **LLaMA**
- Streamlit app for live Q&A with multiple questions
- Modular code for easy reuse

---

## ğŸ’¡ Example: Streamlit App in Action

Once you run the Streamlit app, youâ€™ll see an interface like this:
![RAG PDF Q&A Example](assets/app_example.png)

Ask questions about your PDF (for example):

## Requirements

- Python 3.10+
- Libraries (can be installed via `requirements.txt`):

```bash
pip install -r requirements.txt
```
---

## Project Structure
```
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vector_store/        # Vector DB storage (ignored by git)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Dolphin3.0-Llama3.2-1B-Q4_K_M.gguf   # Local LLaMA model (ignored by git)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest_pdf.py        # Load PDF and extract text
â”‚   â”œâ”€â”€ split_text.py        # Split text into chunks
â”‚   â”œâ”€â”€ create_vectorstore.py # Create embeddings & Chroma vector store
â”‚   â”œâ”€â”€ RAG_pipeline.py      # Main functions: ask_question, load_llama_model
â”‚   â””â”€â”€ app.py               # Streamlit app
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## Project Setup
### 1. Clone the repository

```bash
git clone https://github.com/Hashem-Qaryouti/llama3-rag-pipeline.git
cd <your-repo-name>
```
### 2. Create and activate a virtual environment
```bash
python -m venv llama_env
```
#### Activate the environment:
- **Windows:**
```bash
llama_env\Scripts\activate
```
- **Linux:**
```bash
source llama_env/bin/activate
```
### 3. Install dependencies
```bash
pip install -r requirements.txt

```

### 4. Add your PDF
Place your PDF file in the `data/` folder.
Update paths.py or the PDF_PATH variable in `ingest_pdf.py` to point to your PDF file.

### 5. Add your LLaMA model

Place your downloaded LLaMA GGUF model in the `models/` folder.
Example:
```bash
models/Dolphin3.0-Llama3.2-1B-Q4_K_M.gguf
```
---

## Usage
1. Run the Streamlit App
```bash
cd src
streamlit run app.py
```
- **Open the URL shown in your terminal (usually http://localhost:8501):**
- **Enter your question in the text box and click Ask**
- **View the answer generated by LLaMA based on your PDF content**
